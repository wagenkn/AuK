{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymptotische Aufwandsordnungen\n",
    "\n",
    "## Groß-O\n",
    "\n",
    "Die Groß-O ($\\mathcal{O}$) Notation ist eine mathematische Notation, die das Verhalten einer Funktion für größer werdende Argumente beschreibt. Der sog. $\\mathcal{O}$-Kalkül gehört zur Gruppe der Notationen, welche durch Paul Bachmann und Edmund Landau eingeführt wurden, und deshalb auch Bachmann-Landau Notationen genannt werden.\n",
    "\n",
    "In der Informatik wird die Groß-O Notation genutzt, um Algorithmen bezüglich ihrer Laufzeit und ihres Speicherbedarfs zu klassifizieren.\n",
    "\n",
    "__Anmerkung__\n",
    "\n",
    "In den folgenden Definitionen wird traditionell $f(n)$ auch dann geschrieben, wenn es sich nicht um einen Funktionswert, sondern um eine einstellige Funktion $f$ mit $n\\mapsto f(n)$ handelt.\n",
    "\n",
    "__Definition 1.1__\n",
    "$$\n",
    "f(n) \\in \\mathcal{O}(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\ \n",
    "\\exists c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : f(n) \\leqslant c \\cdot g(n)\n",
    "$$\n",
    "\n",
    "Diese Definition sagt aus, dass $f$ genau dann zur Menge $\\mathcal{O}(g)$ gehört, wenn es eine positive reelle Zahl $c$ gibt, sodass alle $f(n)$ ab einem gewissen $n_0$ kleiner sind als $c \\cdot g(n)$. Die Funktion $g$, welche durch die Landau-Notation angegeben wird, dient also als obere Schranke für die Funktion $f$.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/89/Big-O-notation.png\" alt=\"Drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "Da es sich um eine obere Schranke handelt, gilt beispielsweise: $\\mathcal{O}(n) \\subset \\mathcal{O}(n^2)$ oder $\\mathcal{O}(n^2) \\subset \\mathcal{O}(n^3)$ oder $\\mathcal{O}(n^2) \\subset \\mathcal{O}(2^n)$. Es ist also auch korrekt anstatt $\\mathcal{O}(n)$, $\\mathcal{O}(n^2)$ anzugeben, dies wäre jedoch für die Praxis nicht sehr sinnvoll.\n",
    "\n",
    "Typische Laufzeiten sind: $\\mathcal{O}(1)$, $\\mathcal{O}(\\log n)$, $\\mathcal{O}(n)$, $\\mathcal{O}(n \\log n)$, $\\mathcal{O}(n^2)$, $\\mathcal{O}(n^3)$, $\\mathcal{O}(2^n)$ und $\\mathcal{O}(n!)$. Es gibt jedoch unendlich viele weitere mögliche sog. __Komplexitätsklassen__. Außerdem muss es sich nicht immer um die Variable $n$ handeln, es können auch mehrere Variablen innerhalb einer Komplexitätsklasse vorkommen. Zum Beispiel ist der Aufwand, um eine Wand der Höhe $h$ und der Breite $b$ zu bemalen, $\\mathcal{O}(hb)$. Hier befassen wir uns jedoch nur mit Zeitaufwänden in Abhängigkeit von einer Problemgröße $n$.\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*yekzNjsqZzGCET2KotEROQ.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "## Groß-Omega\n",
    "\n",
    "Die Groß-Omega ($\\Omega$) Notation ähnelt Groß-O konzeptionell, beschreibt jedoch eine untere Schranke. Die formale Defintion lautet demnach folgendermaßen:\n",
    "\n",
    "__Definition 1.2__\n",
    "$$\n",
    "f(n) \\in \\Omega(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "\\exists c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : f(n) \\geqslant c \\cdot g(n)\n",
    "$$\n",
    "\n",
    "\n",
    "$f$ gehört genau dann zu $\\Omega(g)$, wenn es ein $c$ gibt, für das alle $f(n)$ ab einem bestimmten $n_0$ größer sind als $c \\cdot g(n)$.\n",
    "\n",
    "## Groß-Theta\n",
    "\n",
    "Im Idealfall kann man eine asymptotische Beschränkung nach oben und unten durch ein und dieselbe Funktion mit verschiedenen Faktoren angeben. Grafisch wirkt dies wie ein Band, in dem die Graphen sämtlicher Funktionen aus $\\Theta(g)$ verlaufen. $\\Theta$ beschreibt damit die exakte Komplexitätsklasse.\n",
    "\n",
    "__Definition 1.3__\n",
    "$$\n",
    "f(n) \\in \\Theta(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "\\exists c_1, c_2 \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : c_1 \\cdot f(n) \\leqslant f(n) \\leqslant c_2 \\cdot g(n)\n",
    "$$\n",
    "\n",
    "bzw.\n",
    "\n",
    "$$\n",
    "f(n) \\in \\Theta(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "f(n) \\in \\mathcal{O}(g(n)) \\text{ und } f(n) \\in \\Omega(g(n))\n",
    "$$\n",
    "\n",
    "## Klein-O\n",
    "\n",
    "Die Klein-O ($\\mathcal{o}$) Notation gibt wie die Groß-O Notation eine obere Schranke an. Jedoch handelt es sich hierbei um eine strikte obere Schranke, d.h. für alle Konstanten $c$ ist die Funktion asymptotisch nach oben beschränkt, also ergibt sich folgende Definition:\n",
    "\n",
    "__Definition 1.4__\n",
    "$$\n",
    "f(n) \\in \\mathcal{o}(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\ \n",
    "\\forall c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : f(n) < c \\cdot g(n)\n",
    "$$\n",
    "\n",
    "Dies hat zur Folge, dass $n^2 \\notin \\mathcal{o}(n^2)$, aber $n^2 \\in \\mathcal{o}(n^3)$.\n",
    "\n",
    "## Klein-Omega\n",
    "\n",
    "Analog ist die Klein-Omega ($\\omega$) Notation eine striktere Schranke als $\\Theta$. Folgende Definition ergibt sich:\n",
    "\n",
    "__Definition 1.5__\n",
    "$$\n",
    "f(n) \\in \\omega(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\ \n",
    "\\forall c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : f(n) > c \\cdot g(n)\n",
    "$$\n",
    "\n",
    "Auch hier muss die Ungleichung für alle $c \\in \\mathbb{R}^+$ gelten. Dies hat zur Folge, dass $n^2 \\notin \\omega(n^2)$, aber $n^2 \\in \\mathcal{o}(n)$ oder $n^2 \\in \\mathcal{o}(n \\log n)$.\n",
    "\n",
    "__Anmerkung__\n",
    "\n",
    "In der Praxis wird meistens die $\\mathcal{O}$-Notation verwendet.\n",
    "\n",
    "## Definition über Grenzwerte\n",
    "\n",
    "Es ist auch möglich den Zusammenhang zweier Funktionen bezüglich der Landau-Notationen über den Grenzwert des Quotienten der beiden Funktionen zu definieren.\n",
    "\n",
    "__Definition 1.6__\n",
    "$$\\\\\n",
    "f(n) \\in \\mathcal{O}(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "0 \\leqslant \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} < \\infty\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert endlich, so steigt $f(n)$ asymptotisch höchstens so stark wie $g(n)$.\n",
    "\n",
    "__Definition 1.7__\n",
    "$$\\\\\n",
    "f(n) \\in \\Omega(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "0 < \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)}\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert größer als 0, so steigt $f(n)$ asymptotisch mindestens so stark wie $g(n)$.\n",
    "\n",
    "__Definition 1.8__\n",
    "$$\\\\\n",
    "f(n) \\in \\Theta(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "0 < \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} < \\infty\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert größer als 0 und endlich, so steigt $f(n)$ asymptotisch genauso stark wie $g(n)$.\n",
    "\n",
    "__Definition 1.9__\n",
    "$$\\\\\n",
    "f(n) \\in \\mathcal{o}(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert 0, so steigt $f(n)$ asymptotisch weniger stark als $g(n)$.\n",
    "\n",
    "__Definition 1.10__\n",
    "$$\\\\\n",
    "f(n) \\in \\omega(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert $\\infty$, so steigt $f(n)$ asymptotisch stärker als $g(n)$.\n",
    "\n",
    "\n",
    "## Best Case, Worst Case, Average Case\n",
    "\n",
    "Die Groß-O ($\\mathcal{O}$) Notation wird verwendet, um das Laufzeitverhalten eines Algorithmus zu charakterisieren. Hierfür kommen die folgenden drei Analyseformen in Betracht. \n",
    "\n",
    "Sie unterscheiden sich in der Wahl der jeweiligen Laufzeit $f(n)$ aus einer Liste von $k$ Laufzeiten $\\underbrace{f(n),f(n),\\ldots,f(n)}_{k\\ Stück}$ für eine bestimmte Problemgröße $n$: Greift man bei einer solchen empirischen Analyse für jedes $n$ den kleinsten der $k$ Werte für $f(n)$ heraus, führt dies zu einer Best-case-Analyse. Bildet man das arithmetische Mittel aus den jeweils $k$ Werten für $f(n)$, gelangt man zum Average case. Nimmt man die jeweiligen Maximalwerte, ergibt das den Worst case. In den genannten drei Analyseformen steht am Ende eine den Zusammenhang zwischen $n$ und Zeitaufwand $T=f(n)$ beschreibende Funktion $f$, deren asymptotische Ordnung das gesuchte Ergebnis beschreibt.\n",
    "\n",
    "Alle drei Analyseformen sind entweder Kern einer empirischen Analyse (Datenerhebung) oder verwenden Wahrscheinlichkeiten und erfordern den Einsatz statistischer Methoden. \n",
    "\n",
    "\n",
    "### Best Case\n",
    "\n",
    "Mit der __Best-Case__-Laufzeit wird die schnellstmögliche Laufzeit angegeben. Sie tritt ein, wenn die zu lösende Instanz des Problems der Größe $n$ im Hinblick auf den Sortier(zeit)aufwand am günstigsten ist. Bei *Selection Sort* (03 - Abstrakte Datentypen und Datenstrukturen) beispielsweise ist dies der Fall, wenn das Array (bzw. die Liste) mit den zu sortierenden Elementen bereits sortiert ist. Hierfür müsste lediglich einmal durch das Array traversiert werden und die Best-Case-Laufzeit beträgt somit $\\mathcal{O}(n)$. Da der Best Case in der Praxis so gut wie nie auftritt, ist dessen Angabe nicht von großer Bedeutung.\n",
    "\n",
    "### Worst Case\n",
    "\n",
    "Die __Worst-Case__-Laufzeit ist hingegen wesentlich bedeutender. Sie gibt an, wie groß die Laufzeit des Algorithmus maximal werden kann, auch wenn das zu lösende Problem noch so ungünstig ist. Bei *Selection Sort* (03 - Abstrakte Datentypen und Datenstrukturen) tritt der Worst Case ein, wenn es sich um ein umgekehrt sortiertes Array (z.B. von _groß_ nach _klein_) handelt. In diesem Fall müsste man immer durch das komplette Array gehen und kommt auf einen Aufwand von $\\mathcal{O}(n^2)$.\n",
    "\n",
    "### Average Case\n",
    "\n",
    "Nicht immer ist die Worst-Case-Angabe hilfreich. Was ist, wenn der Worst Case zwar bezüglich der Laufzeit sehr ungünstig ist, jedoch nur sehr selten eintritt? Hier ist die Angabe der Laufzeit im __Average Case__ bzw. __Expected Case__ repräsentativer. Eine Alternative stellt die amortisierte Analyse dar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vernachlässigen der Konstanten\n",
    "\n",
    "Bei Angabe der Komplexitätsklasse unter Verwendung der $\\mathcal{O}$-Notation werden Konstanten als Summanden und Faktoren vernachlässigt. Eine Laufzeit, die als $\\mathcal{O}(2n)$ angegeben wurde, ist tatsächlich $\\mathcal{O}(n)$. Zum einen wird dies getan, da nur die Komplexitätsklasse interessiert, d.h. eine ungefähre Einordnung des Verhalten der Laufzeit (bzw. des Speicheraufwands) in Abhängigkeit von $n$. Zum anderen wäre eine Angabe als $\\mathcal{O}(2n)$ keineswegs genauer, wie folgende Beispiele zeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst1 = [1, 2, 3, 4, 5, 6]\n",
    "lst2 = []\n",
    "lst3 = []\n",
    "\n",
    "for n in lst1:\n",
    "    lst2.append(2 * n)\n",
    "    lst3.append(3 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst1 = [1, 2, 3, 4, 5, 6]\n",
    "lst2 = []\n",
    "lst3 = []\n",
    "\n",
    "for n in lst1:\n",
    "    lst2.append(2 * n)\n",
    "    \n",
    "for n in lst2:\n",
    "    lst3.append(3 * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man könnte nun auf die Idee kommen, für das erste Beispiel $\\mathcal{O}(n)$ und für das zweite Beispiel $\\mathcal{O}(2n)$ anzugeben, da es beim ersten Beispiel eine Schleife mit $n$ Iterationen gibt und im zweiten zwei Schleifen mit $n$ Iterationen. Tatsächlich werden aber in beiden Beispielen gleich viele Operationen durchgeführt und eine Unterscheidung der Laufzeit würde hier keinen Sinn ergeben und zu falschen Schlüssen führen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial vs. Exponentiell\n",
    "\n",
    "Alle Komplexitätsklassen gehören einer der beiden (fundamental unterschiedlichen) Aufwandsordnungen an: solche mit polynomialem und solche mit exponentiellem Aufwand.\n",
    "\n",
    "__Satz 1.11__ Lässt sich der Aufwand als Polynom in $n$ ausdrücken, so handelt es sich um polynomialen Aufwand. \n",
    "\n",
    "Das Polynom hat folgende Form:\n",
    "\n",
    "$$T(n) = \\sum_{i=0}^r \\left(a_i n^i\\right) = a_r n^r + a_{r-1} n^{r-1} + \\dotsc + a_1 n + a_0$$\n",
    "\n",
    "$$\\text{mit } r \\in \\mathbb{N}, a_0, \\dotsc, a_r \\in \\mathbb{R}, a_r \\neq 0$$\n",
    "\n",
    "__Satz 1.12__ Da Logarithmus- und Wurzel-Funktionen, bzw. Funktionen mit nicht-ganzzahligen Exponenten, durch Polynome nach oben beschränkt werden können, zählen diese Funktionen auch zu den Polynomialfunktionen.\n",
    "\n",
    "__Satz 1.13__ Lässt sich der Aufwand $T$ nicht als Polynom, sondern in der Gestalt $T(n) = c \\cdot z^n$, mit $c, z \\in \\mathbb{R}$, $c \\neq 0$ und $z > 1$, angeben, so spricht man von exponentiellem Aufwand.\n",
    "\n",
    "Für die Praxis ist diese Einteilung entscheidend, da man Algorithmen mit exponentiellen Aufwand schon für relativ kleine und erst recht für größere $n$ als nicht praktikabel einstufen muss. Bei Algorithmen mit exponentiellem Zeitaufwand steigt die benötigte Zeit mit größer werdenden $n$ so gigantisch an, dass man in der Größenordnung von Jahrmillionen auf das Ergebnis warten müsste, sodass der Algorithmus nutzlos ist.\n",
    "\n",
    "Um dies zu verdeutlichen, vergleichen wir die Dauer der Berechnungen bei $T_1(n) = 10000 \\cdot n^2$ und $T_2(n) = 2^n$, unter der Annahme, dass der Computer $10^9$ Operationen in der Sekunde durchführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        T1(n)                  T2(n)\n",
      "5    0.00025s                   0.0s\n",
      "10     0.001s                   0.0s\n",
      "15   0.00225s                 3e-05s\n",
      "20     0.004s               0.00105s\n",
      "25   0.00625s               0.03355s\n",
      "30     0.009s               1.07374s\n",
      "35   0.01225s              34.35974s\n",
      "40     0.016s            18.32519min\n",
      "45   0.02025s               9.77344h\n",
      "50     0.025s                    13d\n",
      "55   0.03025s                   1yrs\n",
      "60     0.036s                  36yrs\n",
      "65   0.04225s               1,169yrs\n",
      "70     0.049s              37,436yrs\n",
      "75   0.05625s           1,197,962yrs\n",
      "80     0.064s          38,334,786yrs\n",
      "85   0.07225s       1,226,713,160yrs\n",
      "90     0.081s      39,254,821,134yrs\n",
      "95   0.09025s   1,256,154,276,291yrs\n",
      "100      0.1s  40,196,936,841,331yrs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def T1(n):\n",
    "    return 10000 * (n**2)\n",
    "\n",
    "\n",
    "def T2(n):\n",
    "    return 2**n\n",
    "\n",
    "\n",
    "def time(func, n, ops_per_sec):\n",
    "    ops = func(n)\n",
    "    if ops > sys.float_info.max:\n",
    "        t = ops // ops_per_sec\n",
    "    else:\n",
    "        t = ops / ops_per_sec\n",
    "    if t >= 365 * 24 * 60 * 60:\n",
    "        return '{:,}yrs'.format(round(t // (365 * 24 * 60 * 60)))\n",
    "    if t >= 24 * 60 * 60:\n",
    "        return str(round(t // (24 * 60 * 60))) + 'd'\n",
    "    if t >= 60 * 60:\n",
    "        return str(round((t / (60 * 60)), 5)) + 'h'\n",
    "    if t >= 60:\n",
    "        return str(round((t/60), 5)) + 'min'\n",
    "    return str(round(t, 5)) + 's'\n",
    "\n",
    "\n",
    "n = list(range(5, 101, 5))\n",
    "ops_per_sec = 10**9\n",
    "T1_list = []\n",
    "T2_list = []\n",
    "\n",
    "for i in n:\n",
    "    T1_list.append(time(T1, i, ops_per_sec))\n",
    "    T2_list.append(time(T2, i, ops_per_sec))\n",
    "\n",
    "\n",
    "print(pd.DataFrame({'T1(n)': pd.Series(T1_list, index=n), 'T2(n)': pd.Series(T2_list, index=n)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während $T2$ für sehr kleine Werte günstigere Laufzeiten liefert als $T1$, was zunächst nicht auf einen großen Anstieg der Laufzeit schließen lässt, wird doch relativ schnell klar, wie langsam ein Algorithmus mit $T2$ ist. Bereits für $n=100$ werden über 40 Billionen Jahre Rechenzeit benötigt.\n",
    "\n",
    "__*Beweis.*__ Ferner kann man zeigen, dass jede Exponentialfunktion $f(n) = a^n$ mit $a > 1$ für $n \\to \\infty$ schneller wächst als jede Polynomialfunktion $g(n) = b \\cdot n^c$ mit $c \\in \\mathbb{N}$ für $n \\to \\infty$:\n",
    "\n",
    "$$\n",
    "\\text{Behauptung: } a^n \\in \\omega(b \\cdot n^c) \\\\ \n",
    "a^n \\in \\omega(b \\cdot n^c) \\implies \\lim_{n \\to \\infty} \\frac{a^n}{b \\cdot n^c} = \\infty\n",
    "$$\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\frac{a^n}{b \\cdot n^c} = \\lim_{n \\to \\infty} \\frac{\\mathrm{e}^{\\ln(a) \\cdot n}}{b \\cdot n^c} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot n^c}}{\\frac{1}{\\mathrm{e}^{\\ln(a) \\cdot n}}}$$\n",
    "\n",
    "Da hier sowohl Zähler als auch Nenner gegen 0 streben, kann die Regel von l'Hospital angewandt werden:\n",
    "\n",
    "$$= \\lim_{n \\to \\infty} \\frac{\\frac{d}{d n} \\left(\\frac{1}{b \\cdot n^c} \\right)}{\\frac{d}{d n} \\left(\\frac{1}{\\mathrm{e}^{\\ln(a) \\cdot n}}\\right)} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{\\frac{d}{d n}(b \\cdot n^c)}}{\\frac{1}{\\frac{d}{d n}(\\mathrm{e}^{\\ln(a) \\cdot n})}} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot c \\cdot n^{c-1}}}{\\frac{1}{\\ln(a) \\cdot a^n}} $$\n",
    "\n",
    "Da weiterhin sowohl Zähler als auch Nenner gegen 0 streben, muss die Regel von l'Hospital solange angewandt werden, bis dies bei einem der beiden Terme nicht mehr der Fall ist:\n",
    "\n",
    "$$= \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot c!}}{\\frac{1}{\\ln^c(a) \\cdot a^n}} = \\lim_{n \\to \\infty} \\frac{\\ln^c(a) \\cdot a^n}{b \\cdot c!} = \\infty$$\n",
    "\n",
    "<div style=\"text-align: right; font-size: 24px;\">&#9633;</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
