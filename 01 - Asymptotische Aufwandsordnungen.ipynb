{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effizienz von Programmen\n",
    "\n",
    "## Klassifikation von Problemen\n",
    "\n",
    "1. Probleme, die sich einer Formalisierung entziehen\n",
    "2. Absolut unlösbare Probleme: Halteproblem (FSuA)\n",
    "3. Praktisch unlösbare Probleme: \"Baum aller Schachspiele\"\n",
    "4. Algorithmisch (inkl. mit Formel) *lösbare* Probleme\n",
    "\n",
    "## Effizienzbegriff\n",
    "\n",
    "FALSCH: Computer sind immer schneller geworden, sodass die Effizienz von Programmen keine Rolle spielt\n",
    "\n",
    "RICHTIG: Wir bestimmen den Aufwand (Ressourcenverbrauch: Zeit, Speicher) in Abhängigkeit von der Problemgröße $n$.\n",
    "\n",
    "Zwei grundlegende Fragen, die wir uns in diesem Lehrgebiet stellen:\n",
    "\n",
    "1. Wie können wir für konkrete Programme bestimmen (vorhersagen, i.S. einer Vorab-Leistungsgarantie), welche Rechenzeit sie zur Lösung eines Problems bestimmter Größe benötigen werden?\n",
    "\n",
    "2. Kann man diesen erforderlichen Zeitaufwand für typische algorithmische Entwurfsmuster angeben bzw. begrenzen?\n",
    "\n",
    "Effizienzbegriff Zeit-/Speicherbedarf. Die (Zeit-)**Effizienz** eines Programmes ist umso besser, je *weniger Zeit* es zur Lösung eines Problems benötigt.\n",
    "\n",
    "Wir beschränken uns auf die Betrachtung der **Zeiteffizienz**. Begründung: Trade-off (Tauschgeschäft) zwischen Zeit- und Speichereffizienz. Beispiel \"Fibonacci-Zahlen, rekursiv\": Memoizing kostet zwar Speicher aber reduziert die Rechenzeit, da Mehrfachberechnungen vermieden werden. => Es reicht, sich nur mit der Zeiteffizienz zu beschäftigen.\n",
    "\n",
    "1. Problem: große Unterschiede zwischen einzelnen Probleminstanzen in Bezug auf die Effizienzaussage (z.B. Suchverfahren, wenn das zu suchende Element ganz am Anfang einer Liste steht; Primzahlprüfung für $n$-stellige natürliche Zahlen)\n",
    "\n",
    "2. Problem: Einfache Lösungen (d.h. schnell ermittelbare Resultate für kleine Problemgrößen) skalieren nicht mit wachsender Problemgröße. D.h. kein linearer Zusammenhang.\n",
    "\n",
    "3. Problem: Wie können wir entscheiden, welche Option für ein Programm die effizienteste ist?\n",
    "\n",
    "###  Wie wird Zeiteffizienz gemessen? \n",
    "#### ... mit einem Timer\n",
    "\n",
    "          time module    import time\n",
    "                         def c_to_f(c): \n",
    "                             return c*9/5+32\n",
    "          start clock    t0 = time.clock()\n",
    "          call function  c_to_f(100000)\n",
    "          step clock     t1 = time-clock() - t0\n",
    "                         print(\"t= \", t, \": \", t1, \"s, \")\n",
    "                    \n",
    "Nachteile:\n",
    "- Zeitmessung ist inkonsistent\n",
    "- Ziel: verschiedene Algorithmen bewerten\n",
    "- Laufzeit variiert zwischen Alg., zwischen Implementationen, zwischen Computern auf denen sie laufen\n",
    "- Ergebnisse sind unter Verwendung kleinerer Inputs nicht vorhersagbar.\n",
    "- keine Aussage zum Verhältnis zwischen Eingabe und Rechenzeit\n",
    "\n",
    "#### ... durch Zählen der Operationen\n",
    "Annahme, dass bestimmte elementare- / atomare Operationen (z.B. math. Op., Zuweisungen, Zugriff auf Obj., Vergleiche) konstante Zeit beanspruchen. Dann zählt man die Operationen, die in Abhängigkeit von der Inputgröße ausgeführt werden.\n",
    "\n",
    "        def mysum(x):\n",
    "            total = 0                   1 Operation\n",
    "            for i in range(x+1):        1 Operation, x-mal                                            \n",
    "                total += i              2 Operationen, x-mal\n",
    "        return total                    ---------------------\n",
    "                                        1 + 3x Operationen\n",
    "\n",
    "Ergebnis: Wenn ich die Eingabe verzehnfache, verdreißigfacht sich die Zahl der Elementaroperationen.\n",
    "\n",
    "Diese Messmethode ist besser als die erste (Zeitmessung), hat Vorteile:\n",
    "- ist unabhängig vom konkreten (vernetzten) Computer, auf dem das Programm läuft\n",
    "- Ergebnis ist eine Relation zwischen Input und Zählung\n",
    "\n",
    "und Nachteile:\n",
    "- ist implementationsabhängig \n",
    "- keine klare Definition, welche Operationen zu zählen sind\n",
    "\n",
    "#### ... als funktionaler Zusammenhang T:n->T(n) und abstrakte Notation für die Aufwandsordnung\n",
    "Suche eines Elements in einer (unsortierten) Liste\n",
    "\n",
    "    def search_for_element(L,e):  #Bester Fall: e ist 1. Listenelement \n",
    "                                  #--> 1 x Zyklus --> konst. Zeit\n",
    "            for i in L:      #Schlechtester Fall: e steht ganz am Ende                       \n",
    "                             #--> |L| x Zyklus --> lineare Zeit\n",
    "                if i == e:\n",
    "                    return True\n",
    "            return False\n",
    "           \n",
    "+ Notation $\\mathcal{O}(n)$ ... *asymptotische Ordnung* ... $n$ ist die Länge der Liste\n",
    "+ Leicht zu übertragen: z.B. zwei ineinander verschachtelte Schleifen, je 1..n: $\\mathcal{O}(n^2)$\n",
    "+ Unabhängigkeit vom Comp., auf dem das Programm läuft\n",
    "+ Takte einer **Turingmaschine** als abstraktes Modell für die **Bit-Komplexität**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymptotische Aufwandsordnungen\n",
    "\n",
    "## Bit-Komplexität\n",
    "\n",
    "Im Gegensatz zum *uniformen Komplexitätsmaß* (Registermaschine) betrachten wir im Allgemeinen die **Bit-Komplexität** (Turingmaschine). In der Berechenbarkeitstheorie werden die zugehörigen Maschinenmodelle behandelt. Bei der Bit-Komplexität werden die Problemgrößen durch die Stelligkeit der verwendeten Eingaben bestimmt. \n",
    "\n",
    "Wie eine einfache Rechnung zeigt, spielt die Wahl der Zahlenbasis dabei keine Rolle, da sie die Stelligkeit einer natürlichen Zahl lediglich durch einen *konstanten Faktor* verändert.\n",
    "\n",
    "Für eine höchstens $m$-stellige natürliche Zahl $n$ im $10$er System, gilt $n\\leq 10^{m}-1$. Für eine höchstens $p$-stellige Dualzahl $n$ gilt $n\\leq 2^p-1$. Aus $2^p-1=10^m-1$, mit $m,p\\in \\mathbb{N}$ und $0\\leq m,p$, folgt $3m<p<4m$.\n",
    "\n",
    "Die Stelligkeit von $n$ im Dualsystem ist also durch $4$ mal deren Stelligkeit im Dezimalsystem beschränkt. Ist $n$ höchstens 3-stellig, d.h. $n\\leq 999$, so ist $n$ im Dualsystem höchstens $11$-stellig: $n=1111100111$ ($10$-stellig), s. <a href=\"https://www.matheretter.de/rechner/zahlenkonverter\">Konverter</a>.\n",
    "\n",
    "Für die Bit-Komplexität wird die Eingabe im Allgemeinen als *Dualzahlwort* auf das Band der Turingmaschine geschrieben.\n",
    "\n",
    "\n",
    "## Groß-O\n",
    "\n",
    "Die Groß-O ($\\mathcal{O}$) Notation ist eine mathematische Notation, die das Verhalten einer Funktion für größer werdende Argumente beschreibt. Der sog. **$\\mathcal{O}$-Kalkül** gehört zur Gruppe der Notationen, welche durch Paul Bachmann und Edmund Landau eingeführt wurden, und deshalb auch Bachmann-Landau Notationen genannt werden.\n",
    "\n",
    "In der Informatik wird die Groß-O Notation genutzt, um Algorithmen bezüglich ihrer Laufzeit und ihres Speicherbedarfs zu klassifizieren.\n",
    "\n",
    "__Anmerkung__\n",
    "\n",
    "In den folgenden Definitionen wird traditionell $f(n)$ auch dann geschrieben, wenn es sich nicht um einen Funktionswert, sondern um eine einstellige Funktion $f$ mit $n\\mapsto f(n)$ handelt.\n",
    "\n",
    "__Definition 1.1__\n",
    "$$\n",
    "f(n) \\in \\mathcal{O}(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\ \n",
    "\\exists c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : f(n) \\leqslant c \\cdot g(n)\n",
    "$$\n",
    "\n",
    "Diese Definition sagt aus, dass $f$ genau dann zur Menge $\\mathcal{O}(g)$ gehört, wenn es eine positive reelle Zahl $c$ gibt, sodass alle $f(n)$ ab einem gewissen $n_0$ kleiner sind als $c \\cdot g(n)$. Die Funktion $g$, welche durch die Landau-Notation angegeben wird, dient also als obere Schranke für die Funktion $f$.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/89/Big-O-notation.png\" alt=\"Drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "Da es sich um eine obere Schranke handelt, gilt beispielsweise: $\\mathcal{O}(n) \\subset \\mathcal{O}(n^2)$ oder $\\mathcal{O}(n^2) \\subset \\mathcal{O}(n^3)$ oder $\\mathcal{O}(n^2) \\subset \\mathcal{O}(2^n)$. Es ist also auch korrekt anstatt $\\mathcal{O}(n)$, $\\mathcal{O}(n^2)$ anzugeben, dies wäre jedoch für die Praxis nicht sehr sinnvoll.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typische Laufzeiten sind: \n",
    "    \n",
    "Effizienzklasse             | Beschreibung\n",
    ":-------------------------- | :--------------------------------------------------\n",
    "$\\mathcal{O}(1)$            | konstante Laufzeit (unabh. von Eingabegröße)\n",
    "$\\mathcal{O}(\\log n)$       | logarithmische Laufzeit \n",
    "$\\mathcal{O}(n)$            | lineare Laufzeit, z.B. unverschachtelte Iteration\n",
    "$\\mathcal{O}(n\\log n)$      | logarithmisch-lineare Laufzeit\n",
    "$\\mathcal{O}(n^c)$          | polynomiale Laufzeit (c=2: quadratisch, c=3: kubisch), z.B. verschachtelte Schleifen\n",
    "$\\mathcal{O}(c^n), c>1$     | exponentielle Laufzeit, z.B. mehrfach rekursive Funktionen\n",
    "$\\mathcal{O}(n!)$           | exponentielle Laufzeit, Stirlingsche Formel: $n!\\approx\\sqrt{2\\pi n}\\left(\\frac{n}{e}\\right)^n, n\\rightarrow\\infty$\n",
    "\n",
    "Es gibt viele weitere sog. __Komplexitätsklassen__. Außerdem muss es sich nicht immer um die Variable $n$ handeln, es können auch mehrere Variablen innerhalb einer Komplexitätsklasse vorkommen. Zum Beispiel ist der Aufwand, um eine Wand der Höhe $h$ und der Breite $b$ zu bemalen, $\\mathcal{O}(hb)$. Hier befassen wir uns jedoch nur mit Zeitaufwänden in Abhängigkeit von einer Problemgröße $n$.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*yekzNjsqZzGCET2KotEROQ.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "## Groß-Omega\n",
    "\n",
    "Die Groß-Omega ($\\Omega$) Notation ähnelt Groß-O konzeptionell, beschreibt jedoch eine untere Schranke. Die formale Defintion lautet demnach folgendermaßen:\n",
    "\n",
    "__Definition 1.2__\n",
    "$$\n",
    "f(n) \\in \\Omega(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "\\exists c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : f(n) \\geqslant c \\cdot g(n)\n",
    "$$\n",
    "\n",
    "\n",
    "$f$ gehört genau dann zu $\\Omega(g)$, wenn es ein $c$ gibt, für das alle $f(n)$ ab einem bestimmten $n_0$ größer sind als $c \\cdot g(n)$.\n",
    "\n",
    "## Groß-Theta\n",
    "\n",
    "Im Idealfall kann man eine asymptotische Beschränkung nach oben und unten durch ein und dieselbe Funktion mit verschiedenen Faktoren angeben. Grafisch wirkt dies wie ein Band, in dem die Graphen sämtlicher Funktionen aus $\\Theta(g)$ verlaufen. $\\Theta$ beschreibt damit die exakte Komplexitätsklasse.\n",
    "\n",
    "__Definition 1.3__\n",
    "$$\n",
    "f(n) \\in \\Theta(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "\\exists c_1, c_2 \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : c_1 \\cdot f(n) \\leqslant f(n) \\leqslant c_2 \\cdot g(n)\n",
    "$$\n",
    "\n",
    "bzw.\n",
    "\n",
    "$$\n",
    "f(n) \\in \\Theta(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "f(n) \\in \\mathcal{O}(g(n)) \\text{ und } f(n) \\in \\Omega(g(n))\n",
    "$$\n",
    "\n",
    "## Klein-O\n",
    "\n",
    "Die Klein-O ($\\mathcal{o}$) Notation gibt wie die Groß-O Notation eine obere Schranke an. Jedoch handelt es sich hierbei um eine strikte obere Schranke, d.h. für alle Konstanten $c$ ist die Funktion asymptotisch nach oben beschränkt, also ergibt sich folgende Definition:\n",
    "\n",
    "__Definition 1.4__\n",
    "$$\n",
    "f(n) \\in \\mathcal{o}(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\ \n",
    "\\forall c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : f(n) < c \\cdot g(n)\n",
    "$$\n",
    "\n",
    "Dies hat zur Folge, dass $n^2 \\notin \\mathcal{o}(n^2)$, aber $n^2 \\in \\mathcal{o}(n^3)$.\n",
    "\n",
    "## Klein-Omega\n",
    "\n",
    "Analog ist die Klein-Omega ($\\omega$) Notation eine striktere Schranke als $\\Theta$. Folgende Definition ergibt sich:\n",
    "\n",
    "__Definition 1.5__\n",
    "$$\n",
    "f(n) \\in \\omega(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\ \n",
    "\\forall c \\in \\mathbb{R}^+ : \\exists n_0 \\in \\mathbb{N} : \\forall n \\geqslant n_0 : f(n) > c \\cdot g(n)\n",
    "$$\n",
    "\n",
    "Auch hier muss die Ungleichung für alle $c \\in \\mathbb{R}^+$ gelten. Dies hat zur Folge, dass $n^2 \\notin \\omega(n^2)$, aber $n^2 \\in \\mathcal{o}(n)$ oder $n^2 \\in \\mathcal{o}(n \\log n)$.\n",
    "\n",
    "__Anmerkung__\n",
    "\n",
    "In der Praxis wird meistens die $\\mathcal{O}$-Notation verwendet.\n",
    "\n",
    "## Definition über Grenzwerte\n",
    "\n",
    "Es ist auch möglich den Zusammenhang zweier Funktionen bezüglich der Landau-Notationen über den Grenzwert des Quotienten der beiden Funktionen zu definieren.\n",
    "\n",
    "__Definition 1.6__\n",
    "$$\\\\\n",
    "f(n) \\in \\mathcal{O}(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "0 \\leqslant \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} < \\infty\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert endlich, so steigt $f(n)$ asymptotisch höchstens so stark wie $g(n)$.\n",
    "\n",
    "__Definition 1.7__\n",
    "$$\\\\\n",
    "f(n) \\in \\Omega(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "0 < \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)}\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert größer als 0, so steigt $f(n)$ asymptotisch mindestens so stark wie $g(n)$.\n",
    "\n",
    "__Definition 1.8__\n",
    "$$\\\\\n",
    "f(n) \\in \\Theta(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "0 < \\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} < \\infty\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert größer als 0 und endlich, so steigt $f(n)$ asymptotisch genauso stark wie $g(n)$.\n",
    "\n",
    "__Definition 1.9__\n",
    "$$\\\\\n",
    "f(n) \\in \\mathcal{o}(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = 0\n",
    "$$\n",
    "\n",
    "Ist der Grenzwert 0, so steigt $f(n)$ asymptotisch weniger stark als $g(n)$.\n",
    "\n",
    "__Definition 1.10__\n",
    "$$\\\\\n",
    "f(n) \\in \\omega(g(n)) \\text{ mit } n \\rightarrow \\infty \\text{ genau dann, wenn } \\\\\n",
    "\\lim_{n \\to \\infty} \\frac{f(n)}{g(n)} = \\infty\n",
    "$$\n",
    "\n",
    "Ist $\\frac{f(n)}{g(n)}$ divergent ($\\infty$) für $n\\to\\infty$, so steigt $f(n)$ asymptotisch stärker als $g(n)$.\n",
    "\n",
    "\n",
    "## Best Case, Worst Case, Average Case\n",
    "\n",
    "Die Groß-O ($\\mathcal{O}$) Notation wird verwendet, um das Laufzeitverhalten eines Algorithmus zu charakterisieren. Hierfür kommen die folgenden drei Analyseformen in Betracht: worst case, average case und best case. \n",
    "\n",
    "Sie unterscheiden sich in der Wahl der jeweiligen Laufzeit $f(n)$ aus einer Liste von $k$ Laufzeiten $\\underbrace{f(n),f(n),\\ldots,f(n)}_{k\\ Stück}$ für eine bestimmte Problemgröße $n$: Greift man bei einer solchen empirischen Analyse für jedes $n$ den kleinsten der $k$ Werte für $f(n)$ heraus, führt dies zu einer Best-case-Analyse. Bildet man das arithmetische Mittel aus den jeweils $k$ Werten für $f(n)$, gelangt man zum Average case. Nimmt man die jeweiligen Maximalwerte, ergibt das den Worst case. In den genannten drei Analyseformen steht am Ende eine den Zusammenhang zwischen $n$ und Zeitaufwand $T=f(n)$ beschreibende Funktion $f$, deren asymptotische Ordnung das gesuchte Ergebnis beschreibt.\n",
    "\n",
    "Alle drei Analyseformen sind entweder Kern einer empirischen Analyse (Datenerhebung) oder verwenden Wahrscheinlichkeiten und erfordern den Einsatz statistischer Methoden. \n",
    "\n",
    "Wir illustrieren die drei Fälle am Beispiel von **Monkey sort**, auch Bogosort, stupid sort, slowsort, permutaion sort oder shotgun sort genannt. Es handelt sich um eines der *ineffektivsten* Sortierverfahren, das man sich ausdenken kann.\n",
    "\n",
    "Nehmen wir an, eine Liste mit $n$ natürlichen Zahlen soll aufsteigend sortiert werden. Dann kann man folgendes Verfahren anwenden:\n",
    "1. Falls die Liste bereits sortiert ist, dann STOPP.\n",
    "2. Anderenfalls bringe die Liste durcheinander und gehe zu 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5040:193\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def bogo_sort(L):\n",
    "    anzahl = 0\n",
    "    while not is_sorted(L):\n",
    "        anzahl += 1\n",
    "        random.shuffle(L)       # Shuffle list x in place, and return None.\n",
    "    return anzahl\n",
    "\n",
    "def is_sorted(ls):\n",
    "    return all([ls[i]<ls[i+1] for i in range(len(ls)-1)])\n",
    "\n",
    "L = [6,2,4,12,23,8,9]\n",
    "print(math.factorial(len(L)), end=\":\")  # n! verschiedene Anordnungen gibt es\n",
    "print(bogo_sort(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Best Case\n",
    "\n",
    "Mit der __Best-Case__-Laufzeit wird die kürzeste Laufzeit angegeben. Sie tritt ein, wenn die zu lösende Instanz des Problems der Größe $n$ im Hinblick auf den Sortier(zeit)aufwand am günstigsten ist. Bei *Monkey sort* ist dies der Fall, wenn die Liste mit den zu sortierenden Elementen bereits sortiert ist. Hierfür müsste lediglich genau einmal durch das Array traversiert werden, was einen Aufwand in $\\mathcal{O}(n)$ erfordert. Da der Best Case in der Praxis so gut wie nie auftritt, ist dessen Angabe nicht von großem Interesse.\n",
    "\n",
    "### Worst Case\n",
    "\n",
    "Die __Worst-Case__-Laufzeit ist hingegen wesentlich bedeutender: Sie gibt an, wie groß die Laufzeit des Algorithmus maximal werden kann, auch wenn das betrachtete Probleminstanz noch so ungünstig ist. Bei *Monkey sort* tritt der Worst Case ein, wenn alle $n!$ verschiedenen Anordnungen der Listenelemente mindestens einmal überprüft werden, ob sie aufsteigend sortiert vorliegen. Der Aufwand liegt dann in $\\mathcal{O}(n\\cdot n!)$, kurz: $\\mathcal{O}(n!)$. Dies gilt jedoch nur dann, wenn das Verfahren überhaupt terminiert.\n",
    "\n",
    "### Average Case\n",
    "\n",
    "Nicht immer ist die Worst-Case-Angabe hilfreich. Was ist, wenn der Worst Case zwar bezüglich der Laufzeit sehr ungünstig ist, jedoch nur sehr selten eintritt? Hier ist die Angabe der Laufzeit im __Average Case__ bzw. __Expected Case__ repräsentativer. Eine Alternative stellt die *amortisierte Analyse* dar.\n",
    "\n",
    "Bei *Monkey Sort* handelt es sich um einen *Las-Vegas-Algorithmus*, für den man einen Aufwand in $\\mathcal{O}(n!)$ berechnen kann. Sowohl im Worst Case als auch im Average Case arbeitet das Verfahren mit exponentiellem Aufwand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vernachlässigung von Konstanten\n",
    "\n",
    "Bei Angabe der Komplexitätsklasse unter Verwendung der $\\mathcal{O}$-Notation werden konstante Summanden und konstante Faktoren vernachlässigt. Eine Laufzeit in $\\mathcal{O}(2n)$ liegt tatsächlich in $\\mathcal{O}(n)$. Zum einen wird dies getan, da nur die Komplexitätsklasse interessiert. Zum anderen wäre eine Angabe als $\\mathcal{O}(2n)$ keineswegs genauer, wie folgende Beispiele zeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1 = [1, 2, 3, 4, 5, 6]\n",
    "lst2 = []\n",
    "lst3 = []\n",
    "\n",
    "for n in lst1:\n",
    "    lst2.append(2 * n)\n",
    "    lst3.append(3 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1 = [1, 2, 3, 4, 5, 6]\n",
    "lst2 = []\n",
    "lst3 = []\n",
    "\n",
    "for n in lst1:\n",
    "    lst2.append(2 * n)\n",
    "    \n",
    "for n in lst2:\n",
    "    lst3.append(3 * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man könnte nun auf die Idee kommen, für das erste Beispiel $\\mathcal{O}(n)$ und für das zweite Beispiel $\\mathcal{O}(2n)$ anzugeben, da es beim ersten Beispiel eine Schleife mit $n$ Iterationen gibt und im zweiten zwei Schleifen mit $n$ Iterationen. Tatsächlich werden aber in beiden Beispielen gleich viele Operationen durchgeführt und eine Unterscheidung der Laufzeit würde hier keinen Sinn ergeben und zu falschen Schlüssen führen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel für logarithmische Komplexität\n",
    "\n",
    "Wir betrachten ein Beispielprogramm zur Umwandlung einer natürlichen Zahl in eine Zeichenkette.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12345\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def intToStr(i):\n",
    "    digits = '0123456789'\n",
    "    if i==0:\n",
    "        return '0'\n",
    "    result = ''\n",
    "    while i>0:\n",
    "        result = digits[i%10] + result\n",
    "        i = i // 10\n",
    "    return result\n",
    "\n",
    "print(intToStr(12345))\n",
    "print(intToStr(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Anzahl der Zyklendurchläufe beträgt $k=\\log_{10}i$, denn aus $\\frac{i}{10^k}=1$ folgt $i=10^k$ und damit $k=\\log_{10}i$. Geht man von einem linearen Aufwand in $\\mathcal{O}(1)$ für die Operationen innerhalb der Schleife aus, so ergibt sich für $k$ Durchläufe ein Gesamtaufwand in $\\mathcal{O}(\\log i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial vs. Exponentiell\n",
    "\n",
    "Alle Komplexitätsklassen gehören einer der beiden (fundamental unterschiedlichen) Aufwandsordnungen an: solche mit *polynomialem* und solche mit *exponentiellem* Aufwand.\n",
    "\n",
    "__Satz 1.11__ Lässt sich der Aufwand als Polynom in $n$ ausdrücken, so handelt es sich um polynomialen Aufwand. \n",
    "\n",
    "Das Polynom hat folgende Form:\n",
    "\n",
    "$$T(n) = \\sum_{i=0}^r \\left(a_i n^i\\right) = a_r n^r + a_{r-1} n^{r-1} + \\dotsc + a_1 n + a_0$$\n",
    "\n",
    "$$\\text{mit } r \\in \\mathbb{N}, a_0, \\dotsc, a_r \\in \\mathbb{R}, a_r \\neq 0$$\n",
    "\n",
    "__Satz 1.12__ Da Logarithmus- und Wurzel-Funktionen, bzw. Funktionen mit nicht-ganzzahligen Exponenten, durch Polynome nach oben beschränkt werden können, zählen diese Funktionen auch zu den Polynomialfunktionen.\n",
    "\n",
    "__Satz 1.13__ Lässt sich der Aufwand $T$ nicht als Polynom, sondern in der Gestalt $T(n) = c \\cdot z^n$, mit $c, z \\in \\mathbb{R}$, $c \\neq 0$ und $z > 1$, angeben, so spricht man von exponentiellem Aufwand.\n",
    "\n",
    "Für die Praxis ist diese Einteilung entscheidend, da man Algorithmen mit exponentiellen Aufwand schon für relativ kleine und erst recht für größere $n$ als nicht praktikabel einstufen muss. Bei Algorithmen mit exponentiellem Zeitaufwand steigt die benötigte Zeit mit größer werdenden $n$ so gigantisch an, dass man in der Größenordnung von Jahrmillionen auf das Ergebnis warten müsste, sodass der Algorithmus nutzlos ist.\n",
    "\n",
    "Um dies zu verdeutlichen, vergleichen wir die Dauer der Berechnungen bei $T_1(n) = 10000 \\cdot n^2$ und $T_2(n) = 2^n$, unter der Annahme, dass der Computer $10^9$ Operationen in der Sekunde ausführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        T1(n)                  T2(n)\n",
      "5    0.00025s                   0.0s\n",
      "10     0.001s                   0.0s\n",
      "15   0.00225s                 3e-05s\n",
      "20     0.004s               0.00105s\n",
      "25   0.00625s               0.03355s\n",
      "30     0.009s               1.07374s\n",
      "35   0.01225s              34.35974s\n",
      "40     0.016s            18.32519min\n",
      "45   0.02025s               9.77344h\n",
      "50     0.025s                    13d\n",
      "55   0.03025s                   1yrs\n",
      "60     0.036s                  36yrs\n",
      "65   0.04225s               1,169yrs\n",
      "70     0.049s              37,436yrs\n",
      "75   0.05625s           1,197,962yrs\n",
      "80     0.064s          38,334,786yrs\n",
      "85   0.07225s       1,226,713,160yrs\n",
      "90     0.081s      39,254,821,134yrs\n",
      "95   0.09025s   1,256,154,276,291yrs\n",
      "100      0.1s  40,196,936,841,331yrs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def T1(n):\n",
    "    return 10000 * (n**2)\n",
    "\n",
    "\n",
    "def T2(n):\n",
    "    return 2**n\n",
    "\n",
    "\n",
    "def time(func, n, ops_per_sec):\n",
    "    ops = func(n)\n",
    "    if ops > sys.float_info.max:\n",
    "        t = ops // ops_per_sec\n",
    "    else:\n",
    "        t = ops / ops_per_sec\n",
    "    if t >= 365 * 24 * 60 * 60:\n",
    "        return '{:,}yrs'.format(round(t // (365 * 24 * 60 * 60)))\n",
    "    if t >= 24 * 60 * 60:\n",
    "        return str(round(t // (24 * 60 * 60))) + 'd'\n",
    "    if t >= 60 * 60:\n",
    "        return str(round((t / (60 * 60)), 5)) + 'h'\n",
    "    if t >= 60:\n",
    "        return str(round((t/60), 5)) + 'min'\n",
    "    return str(round(t, 5)) + 's'\n",
    "\n",
    "\n",
    "n = list(range(5, 101, 5))\n",
    "ops_per_sec = 10**9\n",
    "T1_list = []\n",
    "T2_list = []\n",
    "\n",
    "for i in n:\n",
    "    T1_list.append(time(T1, i, ops_per_sec))\n",
    "    T2_list.append(time(T2, i, ops_per_sec))\n",
    "\n",
    "\n",
    "print(pd.DataFrame({'T1(n)': pd.Series(T1_list, index=n), 'T2(n)': pd.Series(T2_list, index=n)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während $T2$ für sehr kleine Werte günstigere Laufzeiten liefert als $T1$, was zunächst nicht auf einen großen Anstieg der Laufzeit schließen lässt, wird doch relativ schnell klar, wie langsam ein Algorithmus mit $T2$ ist. Bereits für $n=100$ werden über 40 Billionen Jahre Rechenzeit benötigt.\n",
    "\n",
    "__*Satz:*__ Jede Exponentialfunktion $f(n) = a^n$ mit $a > 1$ für $n \\to \\infty$ wächst schneller als jede Polynomfunktion $g(n) = b \\cdot n^c$ mit $c \\in \\mathbb{N}$ für $n \\to \\infty$.\n",
    "\n",
    "__*Beweis:*__\n",
    "\n",
    "$$\n",
    "\\text{Behauptung: } a^n \\in \\omega(b \\cdot n^c) \\\\ \n",
    "a^n \\in \\omega(b \\cdot n^c) \\implies \\lim_{n \\to \\infty} \\frac{a^n}{b \\cdot n^c} = \\infty\n",
    "$$\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\frac{a^n}{b \\cdot n^c} = \\lim_{n \\to \\infty} \\frac{\\mathrm{e}^{\\ln(a) \\cdot n}}{b \\cdot n^c} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot n^c}}{\\frac{1}{\\mathrm{e}^{\\ln(a) \\cdot n}}}$$\n",
    "\n",
    "Da hier sowohl Zähler als auch Nenner gegen 0 streben, kann die Regel von l'Hospital angewandt werden:\n",
    "\n",
    "$$= \\lim_{n \\to \\infty} \\frac{\\frac{d}{d n} \\left(\\frac{1}{b \\cdot n^c} \\right)}{\\frac{d}{d n} \\left(\\frac{1}{\\mathrm{e}^{\\ln(a) \\cdot n}}\\right)} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{\\frac{d}{d n}(b \\cdot n^c)}}{\\frac{1}{\\frac{d}{d n}(\\mathrm{e}^{\\ln(a) \\cdot n})}} = \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot c \\cdot n^{c-1}}}{\\frac{1}{\\ln(a) \\cdot a^n}} $$\n",
    "\n",
    "Da weiterhin sowohl Zähler als auch Nenner gegen 0 streben, muss die Regel von l'Hospital solange angewandt werden, bis dies bei einem der beiden Terme nicht mehr der Fall ist:\n",
    "\n",
    "$$= \\lim_{n \\to \\infty} \\frac{\\frac{1}{b \\cdot c!}}{\\frac{1}{\\ln^c(a) \\cdot a^n}} = \\lim_{n \\to \\infty} \\frac{\\ln^c(a) \\cdot a^n}{b \\cdot c!} = \\infty$$\n",
    "\n",
    "<div style=\"text-align: right; font-size: 24px;\">&#9633;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist also von besonderer Bedeutung, wenn es gelingt, für einen Rechenprozess, der mit exponentiellem Zeitaufwand arbeitet, einen alternativen mit polynomialer Effizienz anzugeben. \n",
    "\n",
    "**Beispiel:** Fibonacci-Zahlen (rekursiv: $\\mathcal{O}(2^n)$ und iterativ: $\\mathcal{O}(n)$ im worst und average case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346269\n"
     ]
    }
   ],
   "source": [
    "def fib_rec(n):\n",
    "    if n == 0:      # best case: O(1)\n",
    "        return 1\n",
    "    elif n == 1:    # best case: O(1)\n",
    "        return 1\n",
    "    else:\n",
    "        return fib_rec(n-1) + fib_rec(n-2)  # average/wort case: O(2^n), d.h. exponentiell\n",
    "\n",
    "print(fib_rec(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1346269\n"
     ]
    }
   ],
   "source": [
    "def fib_iter(n):\n",
    "    if n == 0:       # best case: O(1)\n",
    "        return 1 \n",
    "    elif n == 1:     # best case: O(1)\n",
    "        return 1\n",
    "    else:\n",
    "        fib_i = 1\n",
    "        fib_ii = 1\n",
    "        for i in range(n-1):   # average/wort case: O(n), d.h. linear\n",
    "            temp = fib_i\n",
    "            fib_i = fib_ii\n",
    "            fib_ii = temp + fib_ii\n",
    "    return fib_ii\n",
    "\n",
    "print(fib_iter(30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
